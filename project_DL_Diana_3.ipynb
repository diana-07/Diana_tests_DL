{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPXkMLdKDdthgTMPuUYnLU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diana-07/Diana_tests_DL/blob/main/project_DL_Diana_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#eliminar pastas caso seja necessário\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "os.chdir('/content')\n",
        "print(os.getcwd())\n",
        "\n",
        "# Caminho para a pasta a apagar\n",
        "dir_path = \"dataset\"\n",
        "\n",
        "# Só apaga se existir\n",
        "if os.path.exists(dir_path):\n",
        "    shutil.rmtree(dir_path)\n",
        "    print(f\"Pasta '{dir_path}' apagada com sucesso.\")\n",
        "else:\n",
        "    print(f\"Pasta '{dir_path}' não existe.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCpq0-QwjRxv",
        "outputId": "ec2f2d41-ba53-4fb1-82c9-9ca64f8296e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Pasta 'dataset' apagada com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "collapsed": true,
        "id": "JVJwkVEpqVI0",
        "outputId": "af0ceb22-11ae-4bc4-9bb7-19d37378b9f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diretório atual: /content/dataset\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'collect_by_players' from 'prepare_data' (/content/dataset/prepare_data.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9f651eec1ecb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprepare_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollect_by_players\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m# from prepare_data import pad_or_truncate_keypoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# from prepare_data import extract_player_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'collect_by_players' from 'prepare_data' (/content/dataset/prepare_data.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "# Define o diretório dataset como diretório atual\n",
        "os.makedirs(\"dataset\", exist_ok=True)  # cria se não existir\n",
        "\n",
        "with open(\"dataset/prepare_data.py\", \"w\") as f:\n",
        "    f.write(\"# Código Python aqui\\n\")\n",
        "\n",
        "os.chdir(\"dataset\") # considerar diretorio atual de trabalho a pasta dataset\n",
        "\n",
        "print(\"Diretório atual:\", os.getcwd())\n",
        "\n",
        "\n",
        "from prepare_data import collect_by_players\n",
        "# from prepare_data import pad_or_truncate_keypoints\n",
        "# from prepare_data import extract_player_id\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y mediapipe-silicon\n",
        "!pip uninstall -y mediapipe\n",
        "!pip uninstall -y protobuf\n",
        "!pip install protobuf==3.20.3  # Specific version that often works well with MediaPipe\n",
        "!pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "vH6Bd75BvpqR",
        "outputId": "296f3f56-9b27-4760-99cf-3d015a419d89"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping mediapipe-silicon as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: mediapipe 0.10.21\n",
            "Uninstalling mediapipe-0.10.21:\n",
            "  Successfully uninstalled mediapipe-0.10.21\n",
            "Found existing installation: protobuf 4.25.7\n",
            "Uninstalling protobuf-4.25.7:\n",
            "  Successfully uninstalled protobuf-4.25.7\n",
            "Collecting protobuf==3.20.3\n",
            "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "Installing collected packages: protobuf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "3a2822c337344da39ed49b5fdc0815ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Using cached mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Using cached protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.15.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Using cached mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "Using cached protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Installing collected packages: protobuf, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.21 protobuf-4.25.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "mediapipe"
                ]
              },
              "id": "2f56f5d3487341dbb91cf175ae69304a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n"
      ],
      "metadata": {
        "id": "aeZtH0LdvfS7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  !git clone --filter=blob:none --no-checkout https://github.com/THETIS-dataset/dataset.git\n",
        "  #%cd dataset\n",
        "  !git sparse-checkout init --cone\n",
        "  !git sparse-checkout set VIDEO_Skelet3D\n",
        "  !git checkout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT-W0M3lcuyT",
        "outputId": "fc56efc8-0ad9-4bc3-b7ed-079568d0481e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dataset'...\n",
            "remote: Enumerating objects: 113, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 113 (delta 0), reused 2 (delta 0), pack-reused 110 (from 1)\u001b[K\n",
            "Receiving objects: 100% (113/113), 214.90 KiB | 15.35 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n",
            "/content/dataset\n",
            "remote: Enumerating objects: 1217, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 1217 (delta 0), reused 0 (delta 0), pack-reused 1216 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1217/1217), 507.08 MiB | 28.81 MiB/s, done.\n",
            "Updating files: 100% (1218/1218), done.\n",
            "Your branch is up to date with 'origin/main'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python prepare_data.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdHfOrr3azB6",
        "outputId": "bee3e17f-90c7-40f9-e2a2-05458cb64a96"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-10 13:48:58.796811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746884938.825154    6112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746884938.833106    6112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "CUDA available: False\n",
            "Device name: No GPU\n",
            "['backhand2hands', 'forehand_openstands', 'forehand_slice', 'slice_service', 'smash', 'backhand_slice', 'flat_service', 'forehand_flat', 'kick_service', 'backhand', 'backhand_volley', 'forehand_volley']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pastas_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "kS9lDfJKeSi5",
        "outputId": "d00abb23-7c7e-4b8d-b892-f50417145701"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pastas_dir' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7062341c2de0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpastas_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pastas_dir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "with open(\"all_keypoints.pkl\", \"rb\") as f:\n",
        "    all_keypoints = pickle.load(f)\n",
        "\n",
        "with open(\"player_video_map.pkl\", \"rb\") as f:\n",
        "    player_video_map = pickle.load(f)\n",
        "\n",
        "with open(\"train_players.pkl\", \"rb\") as f:\n",
        "    train_players = pickle.load(f)\n",
        "\n",
        "with open(\"val_players.pkl\", \"rb\") as f:\n",
        "    val_players = pickle.load(f)\n",
        "\n",
        "with open(\"test_players.pkl\", \"rb\") as f:\n",
        "    test_players = pickle.load(f)\n",
        "\n",
        "train_set = collect_by_players(train_players, player_video_map, all_keypoints)\n",
        "val_set = collect_by_players(val_players,  player_video_map, all_keypoints)\n",
        "test_set = collect_by_players(test_players,  player_video_map, all_keypoints)\n",
        "\n",
        "print(f\"Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}\")"
      ],
      "metadata": {
        "id": "9DvBd7_WbBg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3RD PHASE - extract the keypoints AND pad/truncate\n",
        "\n",
        "# root_dir = \"npy_videos\"\n",
        "\n",
        "# mp_pose = mp.solutions.pose\n",
        "# pose = mp_pose.Pose(static_image_mode=False) # This allows MediaPipe to track pose across frames, potentially leading to more stable and accurate keypoint estimations.\n",
        "\n",
        "# all_keypoints = {}\n",
        "\n",
        "# # Top-level loop with a single progress bar\n",
        "# action_folders = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "# for action_folder in tqdm(action_folders, desc=\"Processing actions\"):\n",
        "#     action_path = os.path.join(root_dir, action_folder)\n",
        "#     all_keypoints[action_folder] = {}\n",
        "\n",
        "#     video_files = sorted([f for f in os.listdir(action_path) if f.endswith(\".npy\")])\n",
        "#     for video_file in video_files:  # No tqdm here\n",
        "#         video_path = os.path.join(action_path, video_file)\n",
        "#         try:\n",
        "#             sample = np.load(video_path)\n",
        "#             video_keypoints = []\n",
        "\n",
        "#             for frame in sample:\n",
        "#                 frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "#                 results = pose.process(frame_rgb)\n",
        "\n",
        "#                 if results.pose_landmarks:\n",
        "#                     keypoints = [[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]\n",
        "#                 else:\n",
        "#                     keypoints = np.zeros((33, 3)).tolist()\n",
        "\n",
        "#                 video_keypoints.append(keypoints)\n",
        "\n",
        "#             video_keypoints = np.array(video_keypoints)\n",
        "#             video_keypoints = pad_or_truncate_keypoints(video_keypoints, target_length=120)\n",
        "#             all_keypoints[action_folder][video_file] = video_keypoints\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         except Exception as e:\n",
        "#             pass  # Suppress per-video error printing for a clean log\n",
        "\n",
        "# # Final summary\n",
        "# print(\"\\n Summary of extracted keypoints:\")\n",
        "# for action, videos in all_keypoints.items():\n",
        "#     print(f\"- {action}: {len(videos)} videos processed.\")\n",
        "\n",
        "# # Jsut to see the calues for the first one\n",
        "# first_action = list(all_keypoints.keys())[0]\n",
        "# first_video = list(all_keypoints[first_action].keys())[0]\n",
        "# print(f\"Keypoints for the first frame of {first_video} in {first_action}:\")\n",
        "# print(all_keypoints[first_action][first_video][0])\n",
        "\n",
        "# ##############################################################################\n",
        "\n",
        "# # 4TH PHASE - Organize videos by player and split player IDs\n",
        "\n",
        "# # Organize videos by player\n",
        "# player_video_map = defaultdict(list)\n",
        "# for action, videos in all_keypoints.items():\n",
        "#     for video_file in videos:\n",
        "#         player_id = extract_player_id(video_file)\n",
        "#         player_video_map[player_id].append((action, video_file))\n",
        "\n",
        "# # Split player IDs\n",
        "# players = list(player_video_map.keys())\n",
        "# random.shuffle(players)\n",
        "\n",
        "# train_end = int(len(players) * 0.7)\n",
        "# val_end = int(len(players) * 0.85)\n",
        "\n",
        "# train_players = players[:train_end]\n",
        "# val_players = players[train_end:val_end]\n",
        "# test_players = players[val_end:]\n",
        "\n",
        "# ##########################################################################\n",
        "\n",
        "# # 5 TH PHASE - guardar os ficheiros com keypoints,\n",
        "# # layer_video_map e train_players\n",
        "# import pickle\n",
        "# with open(\"all_keypoints.pkl\", \"wb\") as f:\n",
        "#   pickle.dump(all_keypoints, f)\n",
        "# with open(\"player_video_map.pkl\", \"wb\") as f:\n",
        "#   pickle.dump(player_video_map, f)\n",
        "# with open(\"train_players.pkl\", \"wb\") as f:\n",
        "#   pickle.dump(train_players, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "w77Etzi0AcFM",
        "outputId": "4dc4c26a-179d-4bc0-aa01-d5050cc9291d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'npy_videos'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-cc3e1d27e388>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Top-level loop with a single progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0maction_folders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction_folder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_folders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Processing actions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0maction_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'npy_videos'"
          ]
        }
      ]
    }
  ]
}